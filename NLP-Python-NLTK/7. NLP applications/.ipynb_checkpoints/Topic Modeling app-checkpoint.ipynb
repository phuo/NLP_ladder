{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Pics/MLSb-T.png\" width=\"160\">\n",
    "<br><br>\n",
    "<center><u><H1>Topic Modeling</H1></u></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation: pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    #text = text.decode(\"utf8\")\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    stopw = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopw]\n",
    "    # remove words less than three letters\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    #lower capitalization\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # lemmatize\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = [lemma.lemmatize(word) for word in tokens]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                               Text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/spam.csv', encoding='latin-1')\n",
    "df = df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\n",
    "df = df.rename(columns = {'v1':'Class','v2':'Text'})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Text_p'] = df['Text'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopw = stopwords.words('english')\n",
    "doc = [txt for txt in df['Text_p']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split() if word not in stopw] for document in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.846*\"...\" + 0.165*\"lor\" + 0.161*\"call\" + 0.150*\"\\'ll\" + 0.147*\"later\" + 0.129*\"sorry\" + 0.087*\"home\" + 0.084*\"come\" + 0.061*\"get\" + 0.060*\"got\"'),\n",
       " (1,\n",
       "  u'0.473*\"later\" + 0.450*\"\\'ll\" + 0.440*\"sorry\" + 0.431*\"call\" + -0.351*\"...\" + 0.118*\"meeting\" + -0.061*\"lor\" + 0.054*\"free\" + 0.051*\"text\" + 0.050*\"please\"'),\n",
       " (2,\n",
       "  u'0.274*\"come\" + -0.258*\"...\" + 0.238*\"get\" + -0.213*\"later\" + 0.197*\"n\\'t\" + -0.186*\"sorry\" + 0.179*\"time\" + 0.163*\"want\" + 0.161*\"know\" + 0.155*\"home\"'),\n",
       " (3,\n",
       "  u'-0.498*\"call\" + 0.342*\"come\" + -0.295*\"free\" + 0.261*\"\\'ll\" + 0.154*\"later\" + -0.148*\"mobile\" + -0.144*\"please\" + 0.135*\"sorry\" + -0.122*\"prize\" + -0.119*\"claim\"'),\n",
       " (4,\n",
       "  u'-0.614*\"come\" + -0.365*\"home\" + -0.224*\"call\" + 0.201*\"good\" + 0.168*\"n\\'t\" + 0.165*\"know\" + 0.154*\"day\" + -0.152*\"tomorrow\" + -0.137*\"lor\" + 0.118*\"\\'ll\"'),\n",
       " (5,\n",
       "  u'-0.677*\"lor\" + 0.357*\"come\" + -0.247*\"wat\" + 0.187*\"good\" + 0.171*\"...\" + -0.167*\"anything\" + -0.146*\"home\" + 0.122*\"tomorrow\" + 0.099*\"day\" + -0.096*\"get\"'),\n",
       " (6,\n",
       "  u'0.479*\"home\" + 0.306*\"good\" + -0.243*\"right\" + -0.232*\"pls\" + -0.231*\"send\" + -0.230*\"pick\" + -0.216*\"come\" + -0.215*\"message\" + -0.210*\"phone\" + -0.195*\"cant\"'),\n",
       " (7,\n",
       "  u'-0.606*\"home\" + 0.293*\"lor\" + 0.247*\"time\" + 0.231*\"wat\" + -0.196*\"way\" + -0.191*\"send\" + 0.182*\"come\" + -0.177*\"right\" + -0.175*\"pls\" + -0.175*\"pick\"'),\n",
       " (8,\n",
       "  u'-0.369*\"get\" + 0.322*\"good\" + 0.249*\"pls\" + 0.208*\"day\" + 0.207*\"send\" + -0.200*\"n\\'t\" + 0.199*\"message\" + 0.192*\"right\" + 0.192*\"lor\" + 0.185*\"pick\"'),\n",
       " (9,\n",
       "  u'-0.396*\"get\" + 0.382*\"n\\'t\" + 0.333*\"know\" + 0.220*\"want\" + 0.219*\"dont\" + -0.204*\"good\" + -0.196*\"free\" + -0.165*\"\\'ll\" + 0.163*\"call\" + -0.159*\"wat\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.846*\"...\" + 0.165*\"lor\" + 0.161*\"call\" + 0.150*\"\\'ll\" + 0.147*\"later\" + 0.129*\"sorry\" + 0.087*\"home\" + 0.084*\"come\" + 0.061*\"get\" + 0.060*\"got\"'),\n",
       " (1,\n",
       "  u'0.473*\"later\" + 0.450*\"\\'ll\" + 0.440*\"sorry\" + 0.431*\"call\" + -0.351*\"...\" + 0.118*\"meeting\" + -0.061*\"lor\" + 0.054*\"free\" + 0.051*\"text\" + 0.050*\"please\"'),\n",
       " (2,\n",
       "  u'0.274*\"come\" + -0.258*\"...\" + 0.238*\"get\" + -0.213*\"later\" + 0.197*\"n\\'t\" + -0.186*\"sorry\" + 0.179*\"time\" + 0.163*\"want\" + 0.161*\"know\" + 0.155*\"home\"'),\n",
       " (3,\n",
       "  u'-0.498*\"call\" + 0.342*\"come\" + -0.295*\"free\" + 0.261*\"\\'ll\" + 0.154*\"later\" + -0.148*\"mobile\" + -0.144*\"please\" + 0.135*\"sorry\" + -0.122*\"prize\" + -0.119*\"claim\"'),\n",
       " (4,\n",
       "  u'-0.614*\"come\" + -0.365*\"home\" + -0.224*\"call\" + 0.201*\"good\" + 0.168*\"n\\'t\" + 0.165*\"know\" + 0.154*\"day\" + -0.152*\"tomorrow\" + -0.137*\"lor\" + 0.118*\"\\'ll\"')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'call', 0.0038098807421786994, u'min', 0.0032436961375962451, u'day', 0.0031097048674863671, u'text', 0.0029562404042129168, u'take', 0.0029258383265700254, u\"n't\", 0.0028184258939820349, u'hour', 0.0027195374193139067, u'ready', 0.0026258504412940012, u'today', 0.0025952255126100812, u'reply', 0.0024432161366122552]\n"
     ]
    }
   ],
   "source": [
    "n_topics = 5\n",
    "for i in range(0, n_topics):\n",
    "    t = lda.show_topic(i, 10)\n",
    "    terms = []\n",
    "    for term in t:\n",
    "        terms.append(term[0])\n",
    "        terms.append(term[1])\n",
    "\n",
    "print(terms)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for topic #0: n't\n",
      "Top 10 terms for topic #0: n't,...\n",
      "Top 10 terms for topic #0: n't,...,like\n",
      "Top 10 terms for topic #0: n't,...,like,get\n",
      "Top 10 terms for topic #0: n't,...,like,get,tell\n",
      "Top 10 terms for topic #0: n't,...,like,get,tell,know\n",
      "Top 10 terms for topic #0: n't,...,like,get,tell,know,one\n",
      "Top 10 terms for topic #0: n't,...,like,get,tell,know,one,well\n",
      "Top 10 terms for topic #0: n't,...,like,get,tell,know,one,well,really\n",
      "Top 10 terms for topic #0: n't,...,like,get,tell,know,one,well,really,together\n",
      "Top 10 terms for topic #1: ...\n",
      "Top 10 terms for topic #1: ...,lor\n",
      "Top 10 terms for topic #1: ...,lor,got\n",
      "Top 10 terms for topic #1: ...,lor,got,come\n",
      "Top 10 terms for topic #1: ...,lor,got,come,yup\n",
      "Top 10 terms for topic #1: ...,lor,got,come,yup,wat\n",
      "Top 10 terms for topic #1: ...,lor,got,come,yup,wat,wan\n",
      "Top 10 terms for topic #1: ...,lor,got,come,yup,wat,wan,dear\n",
      "Top 10 terms for topic #1: ...,lor,got,come,yup,wat,wan,dear,day\n",
      "Top 10 terms for topic #1: ...,lor,got,come,yup,wat,wan,dear,day,tomorrow\n",
      "Top 10 terms for topic #2: come\n",
      "Top 10 terms for topic #2: come,want\n",
      "Top 10 terms for topic #2: come,want,get\n",
      "Top 10 terms for topic #2: come,want,get,home\n",
      "Top 10 terms for topic #2: come,want,get,home,know\n",
      "Top 10 terms for topic #2: come,want,get,home,know,call\n",
      "Top 10 terms for topic #2: come,want,get,home,know,call,good\n",
      "Top 10 terms for topic #2: come,want,get,home,know,call,good,work\n",
      "Top 10 terms for topic #2: come,want,get,home,know,call,good,work,lunch\n",
      "Top 10 terms for topic #2: come,want,get,home,know,call,good,work,lunch,time\n",
      "Top 10 terms for topic #3: call\n",
      "Top 10 terms for topic #3: call,...\n",
      "Top 10 terms for topic #3: call,...,later\n",
      "Top 10 terms for topic #3: call,...,later,sorry\n",
      "Top 10 terms for topic #3: call,...,later,sorry,'ll\n",
      "Top 10 terms for topic #3: call,...,later,sorry,'ll,get\n",
      "Top 10 terms for topic #3: call,...,later,sorry,'ll,get,home\n",
      "Top 10 terms for topic #3: call,...,later,sorry,'ll,get,home,good\n",
      "Top 10 terms for topic #3: call,...,later,sorry,'ll,get,home,good,send\n",
      "Top 10 terms for topic #3: call,...,later,sorry,'ll,get,home,good,send,time\n",
      "Top 10 terms for topic #4: call\n",
      "Top 10 terms for topic #4: call,min\n",
      "Top 10 terms for topic #4: call,min,day\n",
      "Top 10 terms for topic #4: call,min,day,text\n",
      "Top 10 terms for topic #4: call,min,day,text,take\n",
      "Top 10 terms for topic #4: call,min,day,text,take,n't\n",
      "Top 10 terms for topic #4: call,min,day,text,take,n't,hour\n",
      "Top 10 terms for topic #4: call,min,day,text,take,n't,hour,ready\n",
      "Top 10 terms for topic #4: call,min,day,text,take,n't,hour,ready,today\n",
      "Top 10 terms for topic #4: call,min,day,text,take,n't,hour,ready,today,reply\n"
     ]
    }
   ],
   "source": [
    "n_topics = 5\n",
    "for i in range(0, n_topics):\n",
    "    t = lda.show_topic(i, 10)\n",
    "    terms = []\n",
    "    for term in t:\n",
    "        terms.append(term[0])\n",
    "        print \"Top 10 terms for topic #\"+ str(i)+ \": \"+ \",\".join(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "\n",
    "https://radimrehurek.com/gensim/install.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
